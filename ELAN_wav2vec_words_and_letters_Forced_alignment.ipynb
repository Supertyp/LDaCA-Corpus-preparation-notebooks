{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96160922-2a66-42a4-b6b5-b75383c08d5b",
   "metadata": {},
   "source": [
    "# wav2vec for ELAN files\n",
    "\n",
    "customized version of \n",
    "https://pytorch.org/audio/main/tutorials/forced_alignment_tutorial.html\n",
    "\n",
    "to run on ELAN files\n",
    "\n",
    "- force align each annotation/wav snippet \n",
    "- save results in json file\n",
    "</br></br>\n",
    "1. some folders are created in workspace\n",
    "</br>\n",
    "2. the wav file is cut into small pieces and saved in folder\n",
    "</br>\n",
    "3. Wav2vec is used to align all sounds and all words\n",
    "</br>\n",
    "4. two tiers are added to each file.\n",
    "   - word tier\n",
    "   - letters tier (for phonemes)\n",
    "   \n",
    "   \n",
    "</br>\n",
    "5. new Elan file is saved\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<div class=\"warning\" style='padding:0.1em; background-color: #FDAE44; color:#51247a; border-style: solid; border-color: #CC5500 '>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'>\n",
    "<b>Never use this script on your main files. always use it on a copy of your files! </b> \n",
    "<br>\n",
    "</p>\n",
    "<p style='margin-left:1em;'></p></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97e6acd-7986-4215-9385-e5952e496018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import IPython\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.random.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7360b7d4-b4bd-478f-8487-c761eb6382c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "model = bundle.get_model().to(device)\n",
    "labels = bundle.get_labels()\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f4905a-8075-419e-80a6-ee2cc89bcc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "folder_path = 'C:\\\\Users\\\\barth\\\\gits\\\\pytorch_wav2vec\\\\test_data\\\\'\n",
    "inputPath = folder_path + \"input\\\\\"\n",
    "mediaPath = folder_path + \"media_snippets\\\\\"\n",
    "\n",
    "def read_json_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading JSON file: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_trellis(emission, tokens, blank_id=0):\n",
    "    num_frame = emission.size(0)\n",
    "    num_tokens = len(tokens)\n",
    "\n",
    "    trellis = torch.zeros((num_frame, num_tokens))\n",
    "    trellis[1:, 0] = torch.cumsum(emission[1:, blank_id], 0)\n",
    "    trellis[0, 1:] = -float(\"inf\")\n",
    "    trellis[-num_tokens + 1 :, 0] = float(\"inf\")\n",
    "\n",
    "    for t in range(num_frame - 1):\n",
    "        trellis[t + 1, 1:] = torch.maximum(\n",
    "            # Score for staying at the same token\n",
    "            trellis[t, 1:] + emission[t, blank_id],\n",
    "            # Score for changing to the next token\n",
    "            trellis[t, :-1] + emission[t, tokens[1:]],\n",
    "        )\n",
    "    return trellis\n",
    "\n",
    "def plot():\n",
    "    fig, ax = plt.subplots()\n",
    "    img = ax.imshow(trellis.T, origin=\"lower\")\n",
    "    ax.annotate(\"- Inf\", (trellis.size(1) / 5, trellis.size(1) / 1.5))\n",
    "    ax.annotate(\"+ Inf\", (trellis.size(0) - trellis.size(1) / 5, trellis.size(1) / 3))\n",
    "    fig.colorbar(img, ax=ax, shrink=0.6, location=\"bottom\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "@dataclass\n",
    "class Point:\n",
    "    token_index: int\n",
    "    time_index: int\n",
    "    score: float\n",
    "\n",
    "def backtrack(trellis, emission, tokens, blank_id=0):\n",
    "    t, j = trellis.size(0) - 1, trellis.size(1) - 1\n",
    "\n",
    "    path = [Point(j, t, emission[t, blank_id].exp().item())]\n",
    "    while j > 0:\n",
    "        # Should not happen but just in case\n",
    "        assert t > 0\n",
    "\n",
    "        # 1. Figure out if the current position was stay or change\n",
    "        # Frame-wise score of stay vs change\n",
    "        p_stay = emission[t - 1, blank_id]\n",
    "        p_change = emission[t - 1, tokens[j]]\n",
    "\n",
    "        # Context-aware score for stay vs change\n",
    "        stayed = trellis[t - 1, j] + p_stay\n",
    "        changed = trellis[t - 1, j - 1] + p_change\n",
    "\n",
    "        # Update position\n",
    "        t -= 1\n",
    "        if changed > stayed:\n",
    "            j -= 1\n",
    "\n",
    "        # Store the path with frame-wise probability.\n",
    "        prob = (p_change if changed > stayed else p_stay).exp().item()\n",
    "        path.append(Point(j, t, prob))\n",
    "\n",
    "    # Now j == 0, which means, it reached the SoS.\n",
    "    # Fill up the rest for the sake of visualization\n",
    "    while t > 0:\n",
    "        prob = emission[t - 1, blank_id].exp().item()\n",
    "        path.append(Point(j, t - 1, prob))\n",
    "        t -= 1\n",
    "\n",
    "    return path[::-1]\n",
    "\n",
    "# Merge the labels\n",
    "@dataclass\n",
    "class Segment:\n",
    "    label: str\n",
    "    start: int\n",
    "    end: int\n",
    "    score: float\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.label}\\t({self.score:4.2f}): [{self.start:5d}, {self.end:5d})\"\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return self.end - self.start\n",
    "\n",
    "\n",
    "def merge_repeats(path):\n",
    "    i1, i2 = 0, 0\n",
    "    segments = []\n",
    "    while i1 < len(path):\n",
    "        while i2 < len(path) and path[i1].token_index == path[i2].token_index:\n",
    "            i2 += 1\n",
    "        score = sum(path[k].score for k in range(i1, i2)) / (i2 - i1)\n",
    "        segments.append(\n",
    "            Segment(\n",
    "                transcript[path[i1].token_index],\n",
    "                path[i1].time_index,\n",
    "                path[i2 - 1].time_index + 1,\n",
    "                score,\n",
    "            )\n",
    "        )\n",
    "        i1 = i2\n",
    "    return segments\n",
    "\n",
    "# Merge words\n",
    "def merge_words(segments, separator=\"|\"):\n",
    "    words = []\n",
    "    sounds = []\n",
    "    i1, i2 = 0, 0\n",
    "    while i1 < len(segments):\n",
    "        \n",
    "        if i2 >= len(segments) or segments[i2].label == separator:\n",
    "            if i1 != i2:\n",
    "                segs = segments[i1:i2]\n",
    "                word = \"\".join([seg.label for seg in segs])\n",
    "                score = sum(seg.score * seg.length for seg in segs) / sum(seg.length for seg in segs)\n",
    "                words.append(Segment(word, segments[i1].start, segments[i2 - 1].end, score))\n",
    "            i1 = i2 + 1\n",
    "            i2 = i1\n",
    "        else:\n",
    "            i2 += 1\n",
    "    print (words, sounds)\n",
    "    return words, sounds\n",
    "\n",
    "def segment_info(i):\n",
    "    ratio = waveform.size(1) / trellis.size(0)\n",
    "    word = word_segments[i]\n",
    "    x0 = int(ratio * word.start)\n",
    "    x1 = int(ratio * word.end)\n",
    "    #print(f\"{word.label} ({word.score:.2f}): {x0 / 44100:.3f} - {x1 / 44100:.3f} sec\")\n",
    "    return ([f\"{word.label}\", format(x0 / 44100,'.3f') , format(x1 / 44100, '.3f')])\n",
    "    \n",
    "\n",
    "for filename in os.listdir(mediaPath):\n",
    "    counter  = 0\n",
    "    \n",
    "    if filename.endswith('.json'):\n",
    "        jsonFile = mediaPath + filename\n",
    "        json_data = read_json_file(jsonFile)\n",
    "        for k, v in json_data.items():\n",
    "            \n",
    "            if v[\"text\"] and len(v[\"text\"]) > 0:\n",
    "                \n",
    "                text_clean = v[\"text\"].upper().replace(' ', '|')\n",
    "                transcript = text_clean\n",
    "            \n",
    "            else:\n",
    "                transcript = \"|\"\n",
    "            #print (text_clean)\n",
    "            \n",
    "            \n",
    "            print (transcript)\n",
    "            regex = re.compile('[^a-zA-Z|]')\n",
    "            transcript = regex.sub('', transcript)\n",
    "            \n",
    "            \n",
    "            print (transcript)\n",
    "            \n",
    "            dictionary = {c: i for i, c in enumerate(labels)}\n",
    "            \n",
    "            tokens = [dictionary[c] for c in transcript]\n",
    "            #print(list(zip(transcript, tokens)))\n",
    "\n",
    "            SPEECH_FILE = \"C:/Users/barth/gits/pytorch_wav2vec/test_data/media_snippets/\" + filename[:-5] + \"\\\\\" + k.replace(\" \", \"_\") + \".wav\" #Cathy_Samun_Wiliang_a1.wav\"\n",
    "            print (SPEECH_FILE)\n",
    "            #bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "            \n",
    "            #model = bundle.get_model().to(device)\n",
    "            #labels = bundle.get_labels()\n",
    "            \n",
    "            \n",
    "            with torch.inference_mode():\n",
    "                waveform, _ = torchaudio.load(SPEECH_FILE)\n",
    "                emissions, _ = model(waveform.to(device))\n",
    "                emissions = torch.log_softmax(emissions, dim=-1)\n",
    "            \n",
    "            emission = emissions[0].cpu().detach()\n",
    "            metadata = torchaudio.info(SPEECH_FILE)\n",
    "            #print(metadata)\n",
    "            \n",
    "            trellis = get_trellis(emission, tokens)\n",
    "           \n",
    "            #plot()\n",
    "            \n",
    "            path = backtrack(trellis, emission, tokens)\n",
    "\n",
    "            segments = merge_repeats(path)\n",
    "            \n",
    "            \n",
    "            letter_list = []\n",
    "\n",
    "            for segment in range(len(segments)):\n",
    "                ratio = waveform.size(1) / trellis.size(0)\n",
    "\n",
    "                x0 = int(ratio * segments[segment].start)\n",
    "                x1 = int(ratio * segments[segment].end)\n",
    "\n",
    "                #print(f\"{word.label} ({word.score:.2f}): {x0 / 44100:.3f} - {x1 / 44100:.3f} sec\")\n",
    "                soundInfo = [f\"{segments[segment].label}\", format(x0 / 44100,'.3f') , format(x1 / 44100, '.3f')]\n",
    "                soundInfo[1] = int(float(soundInfo[1]) * 1000) + v['timeStamp1']\n",
    "                soundInfo[2] = int(float(soundInfo[2]) * 1000) + v['timeStamp1']\n",
    "\n",
    "                # ingnore the word gaps\n",
    "                if soundInfo[0] != \"|\": \n",
    "                    letter_list.append(soundInfo)\n",
    "                \n",
    "                \n",
    "                \n",
    "            word_segments, sounds = merge_words(segments)\n",
    "            word_list = []\n",
    "            for x in range(0, len(word_segments)):\n",
    "                wordInfo = segment_info(x)\n",
    "                wordInfo[1] = int(float(wordInfo[1]) * 1000) + v['timeStamp1']\n",
    "                wordInfo[2] = int(float(wordInfo[2]) * 1000) + v['timeStamp1']\n",
    "                print(wordInfo)\n",
    "                \n",
    "                word_list.append(wordInfo)\n",
    "\n",
    "\n",
    "            print (word_list)\n",
    "            print(v)\n",
    "            v[\"tier_new\"] = v[\"tiername\"] + \"_words\"\n",
    "            v[\"words\"] = word_list\n",
    "            v[\"letters\"] = letter_list                 \n",
    "            \n",
    "            \n",
    "            #print(transcript)\n",
    "            #display_segment(-1)\n",
    "            #IPython.display.Audio(SPEECH_FILE, rate=44100)\n",
    "\n",
    "        print (json_data)\n",
    "        with open(jsonFile, 'w') as json_file:\n",
    "            json.dump(json_data, json_file, indent=2)\n",
    "\n",
    "print (\" +++ DONE +++ \" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649500ae-a261-41f4-8831-3b2525dd6142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_segment(i):\n",
    "    ratio = waveform.size(1) / trellis.size(0)\n",
    "    word = word_segments[i]\n",
    "    x0 = int(ratio * word.start)\n",
    "    x1 = int(ratio * word.end)\n",
    "    print(f\"{word.label} ({word.score:.2f}): {x0 / 44100:.3f} - {x1 / 44100:.3f} sec\")\n",
    "    segment = waveform[:, x0:x1]\n",
    "\n",
    "    return IPython.display.Audio(segment.numpy(), rate=44100)\n",
    "\n",
    "display_segment(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1649a-8938-4990-9317-e2284f327889",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(SPEECH_FILE, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff9c139-de62-4378-81f4-a7497f8086e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"C:\\Users\\barth\\gits\\pytorch_wav2vec\\test_data\\media_snippets\\Cathy_Samun_Wiliang_a1.wav\"\n",
    "\"C:\\Users\\barth\\gits\\pytorch_wav2vec\\test_data\\media_snippets\\Cathy Samun Wiliang_a1.wav\"\n",
    "\n",
    "\"C:\\Users\\barth\\gits\\pytorch_wav2vec\\test_data\\media_snippets\\Cathy_Samun_Wiliang_a53.wav\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
